---
layout: post
title:  "Temel BileÅŸen Analizi (PCA)"
date:   2021-06-20 12:33:07 -0800
categories: yazi
---

![Yosemite - El Capitan](https://miro.medium.com/max/1400/1*HM39uRqzrcGP_6iYd_m73Q.jpeg)

Ã–z Nitelikleri daha kÃ¼Ã§Ã¼k bir alt kÃ¼meye ayrÄ±ÅŸtÄ±rmak iÃ§in birÃ§ok teknik vardÄ±r. Bu yÃ¶ntemler, keÅŸif amaÃ§lÄ± veri analizi, gÃ¶rselleÅŸtirme, tahmine dayalÄ± modeller oluÅŸturma veya kÃ¼meleme problemleri iÃ§in oldukÃ§a faydalÄ± olabilmektedir. Bu yazÄ±da, boyut azaltma iÃ§in Temel BileÅŸen Analizini (PCA) ele alacaÄŸÄ±z. YazÄ±da, hemen her yerde bulabileceÄŸiniz kod Ã¶rneklerinden ziyade konunun matematiksel temellerine deÄŸinilmiÅŸtir.
Temel BileÅŸen Analizi altÄ±nda yatan ana fikir, birbiriyle iliÅŸkili birÃ§ok deÄŸiÅŸkenden oluÅŸan bir veri setinin boyutsallÄ±ÄŸÄ±nÄ±, veri setinde mevcut varyasyonu maksimum Ã¶lÃ§Ã¼de korurken azaltmaktÄ±r. Orijinal verilerin aynÄ± sayÄ±da veya daha az boyutta bir projeksiyonunu hesaplamak iÃ§in lineer cebir ve istatistiksel bazÄ± basit matris iÅŸlemlerini kullanan bir yÃ¶ntemdir.

Uygulamada, ister sÄ±nÄ±flandÄ±rma ister regresyon problemi olsun, bilgi iÃ§erdiÄŸini dÃ¼ÅŸÃ¼ndÃ¼ÄŸÃ¼mÃ¼z gÃ¶zlem verileri girdi olarak alÄ±nÄ±r. Ä°dealde, sÄ±nÄ±flandÄ±rÄ±cÄ± veya regresÃ¶r, bilgi iÃ§ermeyen Ã¶z nitelikleri modelden atarak gerekli olan Ã¶zellikleri kullanabilmelidir dolayÄ±sÄ±yla ayrÄ± bir sÃ¼reÃ§ olarak Ã¶z nitelik seÃ§imine veya Ã§Ä±karÄ±mÄ±na ihtiyaÃ§ duymamamÄ±z gerekir. Ancak, ayrÄ± bir Ã¶n iÅŸleme adÄ±mÄ± olarak boyutluluÄŸu azaltmakla ilgilenmemizin birkaÃ§ nedeni bulunmaktadÄ±r. Bunlar:

- *Ã‡oÄŸu Ã¶ÄŸrenme algoritmasÄ±nda, karmaÅŸÄ±klÄ±k, girdi boyutlarÄ±nÄ±n (d) sayÄ±sÄ±na ve ayrÄ±ca veri Ã¶rneÄŸinin (N) boyutuna baÄŸlÄ±dÄ±r, az maliyetli bellek ve hesaplama iÃ§in, problemin boyutluluÄŸunu azaltmamÄ±z gerekmektedir. Girdi boyutunun (d) azaltÄ±lmasÄ±, test sÄ±rasÄ±nda Ã§Ä±karÄ±m algoritmasÄ±nÄ±n karmaÅŸÄ±klÄ±ÄŸÄ±nÄ±n da azalmasÄ±na,*
- *Bir girdinin gereksiz olduÄŸuna karar verildiÄŸinde, onu Ã§Ä±karma maliyetinden tasarruf edilmesine,*
- *Verilerin daha az Ã¶znitelikle aÃ§Ä±klanabildiÄŸinde, verinin altÄ±nda yatan sÃ¼reÃ§ hakkÄ±nda daha iyi bir fikir edinerek, bilgi Ã§Ä±karÄ±mÄ± yapabilmemize,*
- *Verilerin, bilgi kaybÄ± olmadan birkaÃ§ boyutta temsil edilebilmesi, yapÄ± ve aykÄ±rÄ± deÄŸerler iÃ§in gÃ¶rsel olarak Ã§izilebilir ve analiz edilebilir hale gelmesine olanak tanÄ±maktadÄ±r.*

![Soldaki grafik orijinal verimiz Xâ€™i; SaÄŸdaki grafik ise dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ verimizi temsil eder](https://miro.medium.com/max/756/0*5ITfn8msilGwQ-lN.png)

BoyutluluÄŸu azaltmak iÃ§in iki ana yÃ¶ntem vardÄ±r: Ã¶znitelik seÃ§imi ve Ã¶znitelik Ã§Ä±karma.

- *Ã–znitelik seÃ§iminde, bize en fazla bilgiyi veren d boyuttan k tanesini bulmakla ilgileniyor ve diÄŸer (d-k) boyutlarÄ± atÄ±yoruz.*
- *Ã–znitelik Ã§Ä±karmada ise, orijinal d boyutlarÄ±nÄ±n kombinasyonlarÄ± olan yeni bir k boyutu kÃ¼mesi bulmakla ilgileniyoruz. Bu yÃ¶ntemler, Ã§Ä±ktÄ± bilgilerini kullanÄ±p kullanmamalarÄ±na baÄŸlÄ± olarak denetimli veya denetimsiz olabilir. En iyi bilinen ve en yaygÄ±n olarak kullanÄ±lan Ã¶znitelik Ã§Ä±karma yÃ¶ntemleri, sÄ±rasÄ±yla denetimsiz ve denetimli doÄŸrusal iz dÃ¼ÅŸÃ¼m yÃ¶ntemleri olan Temel BileÅŸenler Analizi (PCA) ve DoÄŸrusal AyÄ±rÄ±cÄ± Analizdir (LDA).*

## Matematiksel Arka Plan

DaÄŸÄ±lÄ±m Ã¶lÃ§Ã¼mlerine veya verilerin nasÄ±l daÄŸÄ±ldÄ±ÄŸÄ±na iliÅŸkin istatistikler ile matrislerin matematiksel arka planlarÄ±, Ã¶z vektÃ¶r, Ã¶z deÄŸerler ve matrislerin PCA iÃ§in temel olan Ã¶nemli Ã¶zellikleri Ã¼zerine GitHub repoma bir bÃ¶lÃ¼m ekledim. Konu ile ilgili detaylÄ± bilgiyi bu baÄŸlantÄ±yÄ± kullanarak ulaÅŸabilirsiniz.

PCA; verilerdeki paterni tanÄ±manÄ±n, verileri benzerliklerini ve farklÄ±lÄ±klarÄ±nÄ± vurgulayacak ÅŸekilde ifade etmenin bir yoludur. Verilerin paternini yÃ¼ksek boyutlu veri setlerinde bulmak zor olabileceÄŸinden, PCA, bizlere verileri analiz etmek iÃ§in gÃ¼Ã§lÃ¼ bir araÃ§ sunmaktadÄ±r. PCAâ€™nÄ±n diÄŸer bir ana avantajÄ±, verilerde bu paterni bulduktan sonra, Ã§ok fazla bilgi kaybÄ± olmadan boyut sayÄ±sÄ±nÄ± azaltarak verileri sÄ±kÄ±ÅŸtÄ±rmanÄ±zdÄ±r.

Hadi bunu bir Ã¶rnek ile aÃ§Ä±klayalÄ±m: Bize altÄ± farklÄ± Ã¶znitelikte derecelendirilmiÅŸ bir mÃ¼ÅŸteri sÄ±nÄ±fÄ± verildiÄŸini ve bu mÃ¼ÅŸterileri derecelerine gÃ¶re sÄ±ralamak istediÄŸimizi varsayalÄ±m. KÄ±sacasÄ±, veri noktalarÄ± arasÄ±ndaki fark en belirgin hale gelecek ÅŸekilde verileri tek bir boyuta yansÄ±tmak istiyoruz. Bu noktada PCAâ€™yÄ± kullanabiliriz. Ã‡Ã¼nkÃ¼, en yÃ¼ksek Ã¶z deÄŸere sahip Ã¶z vektÃ¶r (en yÃ¼ksek varyansa sahip yÃ¶ndÃ¼r), mÃ¼ÅŸterilerin en Ã§ok yayÄ±ldÄ±ÄŸÄ± ekseni temsil eder. Bu durum, temel bileÅŸenler ile temsil edilen varyansÄ± en yÃ¼ksek seviyede tutacak ÅŸekilde veri boyutunu indirgeyebilmemize olanak saÄŸlayacaktÄ±r.

Uygulamada, verilerin sahip olduÄŸu Ã¶zniteliklere ait bazÄ± Ã¶z deÄŸerlerin aÃ§Ä±klanan varyansa Ã§ok az katkÄ±sÄ± olduÄŸunu ve bunlarÄ±n atÄ±labileceÄŸini anlÄ±yoruz. Ã–rneÄŸin aÃ§Ä±klanan varyansÄ±n yÃ¼zde 95'inden fazlasÄ±nÄ± aÃ§Ä±klayan k tane bileÅŸeni dikkate aldÄ±ÄŸÄ±mÄ±zda, Î»iâ€™ler bÃ¼yÃ¼kten kÃ¼Ã§Ã¼ÄŸe sÄ±ralandÄ±ÄŸÄ±nda, k temel bileÅŸenler tarafÄ±ndan aÃ§Ä±klanan varyans oranÄ±:

>(Î» 1 + Î» 2 + â€¦ + Î» k ) / (Î» 1 + Î» 2 + â€¦ + Î» k+ .. + Î» d)

EÄŸer verimize ait Ã¶znitelikler aralarÄ±nda yÃ¼ksek oranda iliÅŸkiliyse, PCA iÅŸlemi sonrasÄ±nda bÃ¼yÃ¼k Ã¶z deÄŸerleri olan az sayÄ±da Ã¶z vektÃ¶r elde etmiÅŸ olacaÄŸÄ±z bu durumda k sayÄ±sÄ±, dâ€™den Ã§ok daha kÃ¼Ã§Ã¼k olacaktÄ±r ve bu sayede boyutlulukta bÃ¼yÃ¼k bir azalma elde edilebilecektir. Bu, tipik olarak, yakÄ±ndaki girdilerin (uzay veya zaman olarak) yÃ¼ksek oranda iliÅŸkili olduÄŸu birÃ§ok gÃ¶rÃ¼ntÃ¼ ve ses iÅŸleme problemi iÃ§in geÃ§erlidir. Boyutlar iliÅŸkili deÄŸilse, k, d kadar bÃ¼yÃ¼k olacaktÄ±r ve PCA yoluyla boyut indirgemede kazanÃ§ olmayacaktÄ±r.

### AdÄ±mlar
Åimdi PCA uygulamamÄ±z iÃ§in izlenmesi gereken adÄ±mlarÄ± teker teker inceleyelim.

#### AdÄ±m 1: Veriyi Normalize Etmek
Ä°lk adÄ±m, PCAâ€™nÄ±n dÃ¼zgÃ¼n Ã§alÄ±ÅŸmasÄ± iÃ§in elimizdeki verileri normalleÅŸtirmektir, burada her bir veri boyutundan ortalamayÄ± Ã§Ä±karmanÄ±z gerekir. Ã‡Ä±karÄ±lan ortalama, her boyuttaki ortalamadÄ±r. BÃ¶ylece, tÃ¼m x deÄŸerlerinin xbâ€™si (tÃ¼m veri noktalarÄ±nÄ±n x deÄŸerlerinin ortalamasÄ±) Ã§Ä±karÄ±lmÄ±ÅŸ ve tÃ¼m y deÄŸerlerinin ybâ€™si onlardan Ã§Ä±karÄ±lmÄ±ÅŸtÄ±r. Bu, ortalamasÄ± sÄ±fÄ±r olan bir veri seti Ã¼retir.

#### AdÄ±m 2: Kovaryans Matrisini HesaplayÄ±n
Bu adÄ±mda yapÄ±lacak iÅŸlemler yukarÄ±da belirttiÄŸim GitHub repomda detaylÄ± ÅŸekilde anlatÄ±lmÄ±ÅŸtÄ±r. Buradaki yÃ¶ntemler incelendiÄŸinde kovaryans matrisinin nasÄ±l hesaplanabileceÄŸi gÃ¶rÃ¼lmÃ¼ÅŸ olacaktÄ±r.

#### AdÄ±m 3: Ã–z deÄŸer ve Ã–z vektÃ¶r Hesaplama
Bu adÄ±mda yapmamÄ±z gereken kovaryans matrisine ait Ã¶z deÄŸer ve Ã¶z vektÃ¶rleri hesaplamaktÄ±r. Æ›, karakteristik denklemin bir Ã§Ã¶zÃ¼mÃ¼ ise, bir A matrisi iÃ§in bir Ã¶z deÄŸeri temsil eder:

>det(Î‘-Î» I)=0

Burada, I matris Ã§Ä±karma iÅŸlemi iÃ§in gerekli bir koÅŸul olan A ile aynÄ± boyuttaki birim matristir ve bu durumda â€œdetâ€ matrisin determinantÄ±dÄ±r. Her bir
Ã¶z deÄŸer Æ› iÃ§in, karÅŸÄ±lÄ±k gelen bir Ã¶z vektÃ¶r v, aÅŸaÄŸÄ±daki eÅŸitliÄŸin Ã§Ã¶zÃ¼lmesi ile bulunabilir:

>Î‘Î½=Î»Î½

#### AdÄ±m 4: BileÅŸenleri SeÃ§me ve Ã–znitelik VektÃ¶rÃ¼ OluÅŸturma
Bu adÄ±mda ise bir Ã¶nceki adÄ±mda hesapladÄ±ÄŸÄ±mÄ±z Ã–z deÄŸerleri bÃ¼yÃ¼kten kÃ¼Ã§Ã¼ÄŸe sÄ±ralarÄ±z, bÃ¶ylece bileÅŸenleri sÄ±ralÄ± veya anlamlÄ± ÅŸekilde elde etmiÅŸ oluruz. Ä°ÅŸte boyut indirgeme kÄ±smÄ± burada devreye giriyor. d deÄŸiÅŸkenli bir veri kÃ¼memiz varsa, buna karÅŸÄ±lÄ±k gelen d Ã¶z deÄŸer ve Ã¶z vektÃ¶re sahibiz demektir. BoyutlarÄ± kÃ¼Ã§Ã¼ltmek iÃ§in ilk k Ã¶z deÄŸeri seÃ§ip gerisini yok sayarÄ±z. SÃ¼reÃ§te bazÄ± bilgileri kaybedebiliriz, ancak yok saydÄ±ÄŸÄ±mÄ±z bu Ã¶z deÄŸerler kÃ¼Ã§Ã¼kse bu kayÄ±p ihmal edilebilir bir kayÄ±p olacaktÄ±r.
Daha sonra, bu Ã¶z vektÃ¶rlerden oluÅŸan bir vektÃ¶r matrisi oluÅŸtururuz. AslÄ±nda, bu sadece devam etmek istediÄŸimiz Ã¶z vektÃ¶rlerdir.

>Feature Vector = (eig1, eig2, â€¦)

#### AdÄ±m 5: Yeni Veri Setini TÃ¼retme
Buraya kadar yaptÄ±ÄŸÄ±mÄ±z tÃ¼m matematiÄŸi kullanarak temel bileÅŸenleri oluÅŸturduÄŸumuz son adÄ±ma geldik. Bu adÄ±mda, Ã¶znitelik vektÃ¶rÃ¼nÃ¼n transpozu ile orijinal verimizin Ã¶lÃ§eklendirilmiÅŸ halinin transpozunu Ã§arpÄ±yoruz.

>New Data = Feature Vector^T x Scaled Data^T

Bir matrisin tÃ¼m Ã¶z vektÃ¶rleri birbirine diktir. DolayÄ±sÄ±yla, PCAâ€™da yaptÄ±ÄŸÄ±mÄ±z ÅŸey, normal x ve y eksenlerinde temsil etmek yerine bu ortogonal (dikey) Ã¶z vektÃ¶rleri kullanarak orijinal veri setini temsil etmek veya dÃ¶nÃ¼ÅŸtÃ¼rmektir.

## SonuÃ§
UmarÄ±m bu yazÄ±mÄ± faydalÄ± bulmuÅŸsunuzdur. PCA konusunda daha detaylÄ± bilgi iÃ§in aÅŸaÄŸÄ±daki referanslarÄ± takip edebilirsiniz. Ä°yi ve saÄŸlÄ±klÄ± gÃ¼nler dilerim. ğŸ˜Š


$x^2=2$

## KaynakÃ§a
- [1] AlpaydÄ±n, E. 2010. â€œIntroduction to Machine Learning.â€The MIT Press Cambridge, Massachusetts, London, England
- [2] Smith, L. 2002. â€œA Tutorial on Principal Components Analysisâ€
- [3] [https://www.dezyre.com/data-science-in-python-tutorial/principal-component-analysis-tutorial](https://www.dezyre.com/data-science-in-python-tutorial/principal-component-analysis-tutorial)
- [4] [https://www.projectrhea.org/rhea/index.php/PCA_Theory_Examples](https://www.projectrhea.org/rhea/index.php/PCA_Theory_Examples)
- [5] [https://images.app.goo.gl/gYxvW9GAVtCv5uzt8](https://images.app.goo.gl/gYxvW9GAVtCv5uzt8)
